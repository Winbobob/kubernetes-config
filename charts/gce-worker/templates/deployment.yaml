{{ if not .Values.cluster.enabled }}
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ include "gce-worker.fullname" . }}
  labels:
    app.kubernetes.io/name: {{ include "gce-worker.name" . }}
    helm.sh/chart: {{ include "gce-worker.chart" . }}
    app.kubernetes.io/instance: {{ .Release.Name }}
    app.kubernetes.io/managed-by: {{ .Release.Service }}
spec:
  replicas: {{ .Values.replicaCount }}
  selector:
    matchLabels:
      app.kubernetes.io/name: {{ include "gce-worker.name" . }}
      app.kubernetes.io/instance: {{ .Release.Name }}
  template:
    metadata:
      labels:
        app.kubernetes.io/name: {{ include "gce-worker.name" . }}
        app.kubernetes.io/instance: {{ .Release.Name }}
    spec:
      containers:
      - name: {{ .Chart.Name }}
        image: "{{ .Values.image.repository }}:{{ .Values.image.tag }}"
        imagePullPolicy: {{ .Values.image.pullPolicy }}
        env:

        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace

        - name: TRAVIS_WORKER_LIBRATO_SOURCE
          value: $(POD_NAMESPACE)-$(POD_NAME)
        - name: TRAVIS_WORKER_HOSTNAME
          value: $(POD_NAME).$(POD_NAMESPACE)

        # The worker secret will have these prefixed with TRAVIS_WORKER,
        # but the AWS library wants these variable names instead.
        - name: AWS_ACCESS_KEY_ID
          value: $(TRAVIS_WORKER_AWS_ACCESS_KEY_ID)
        - name: AWS_SECRET_ACCESS_KEY
          value: $(TRAVIS_WORKER_AWS_SECRET_ACCESS_KEY)

        envFrom:
        - configMapRef:
            name: {{ template "gce-worker.fullname" . }}
        - secretRef:
            name: {{ template "gce-worker.secret" . }}
        volumeMounts:
        - name: gce-json
          mountPath: "/etc/worker/gce/"
          readOnly: true
      volumes:
      - name: gce-json
        secret:
          secretName: {{ template "gce-worker.fullname" . }}-gce-json
      terminationGracePeriodSeconds: 7200
  strategy:
    rollingUpdate:
      # We can probably handle extra jobs while a worker is shutting down,
      # so allow an extra worker to be brought up while one is being shutdown.
      maxUnavailable: 0
      maxSurge: 1

{{ end }}
